{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHL Lymphoma Classification with Hybrid Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow==2.19.0\n",
    "!pip install -q scikit-learn matplotlib seaborn opencv-python-headless Pillow scipy networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"✓ Mixed precision enabled\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import cv2\n",
    "from google.colab import drive\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import cdist\n",
    "from keras.saving import register_keras_serializable\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds and Configure GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✓ GPU available: {len(gpus)} device(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"⚠ No GPU detected. Training will be slower.\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive and Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_PATH = '/content/drive/MyDrive/NHL_DATA/Multi Cancer/Multi Cancer/Lymphoma'\n",
    "\n",
    "if os.path.exists(BASE_PATH):\n",
    "    print(f\"✓ Data directory found: {BASE_PATH}\")\n",
    "    subdirs = [d for d in os.listdir(BASE_PATH) if os.path.isdir(os.path.join(BASE_PATH, d))]\n",
    "    print(f\"  Subdirectories: {subdirs}\")\n",
    "else:\n",
    "    print(f\"✗ Data directory not found: {BASE_PATH}\")\n",
    "    print(\"  Please update BASE_PATH to match your data location\")\n",
    "\n",
    "MODEL_DIR = \"/content/drive/MyDrive/NHL_Project/models\"\n",
    "!mkdir -p $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'IMG_SIZE': (128, 128),\n",
    "    'BATCH_SIZE': 32,\n",
    "    'EPOCHS': 10,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'TRAIN_SPLIT': 0.70,\n",
    "    'VAL_SPLIT': 0.15,\n",
    "    'TEST_SPLIT': 0.15,\n",
    "    'NUM_CLASSES': 3,\n",
    "    'CLASS_NAMES': ['CLL', 'FL', 'MCL'],\n",
    "    'CLASS_LABELS': {\n",
    "        'lymph_cll': 'Chronic Lymphocytic Leukemia (CLL)',\n",
    "        'lymph_fl': 'Follicular Lymphoma (FL)',\n",
    "        'lymph_mcl': 'Mantle Cell Lymphoma (MCL)'\n",
    "    },\n",
    "    'RANDOM_SEED': 42\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(source_dir, dest_dir, train_ratio=0.70, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(dest_dir, split), exist_ok=True)\n",
    "\n",
    "    class_dirs = [d for d in os.listdir(source_dir)\n",
    "                  if os.path.isdir(os.path.join(source_dir, d))]\n",
    "\n",
    "    split_info = {}\n",
    "\n",
    "    for class_dir in class_dirs:\n",
    "        class_path = os.path.join(source_dir, class_dir)\n",
    "        all_files = [f for f in os.listdir(class_path)\n",
    "                    if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "\n",
    "        np.random.shuffle(all_files)\n",
    "\n",
    "        n_total = len(all_files)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "\n",
    "        train_files = all_files[:n_train]\n",
    "        val_files = all_files[n_train:n_train + n_val]\n",
    "        test_files = all_files[n_train + n_val:]\n",
    "\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            os.makedirs(os.path.join(dest_dir, split, class_dir), exist_ok=True)\n",
    "\n",
    "        for fname in train_files:\n",
    "            src = os.path.join(class_path, fname)\n",
    "            dst = os.path.join(dest_dir, 'train', class_dir, fname)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "        for fname in val_files:\n",
    "            src = os.path.join(class_path, fname)\n",
    "            dst = os.path.join(dest_dir, 'val', class_dir, fname)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "        for fname in test_files:\n",
    "            src = os.path.join(class_path, fname)\n",
    "            dst = os.path.join(dest_dir, 'test', class_dir, fname)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "        split_info[class_dir] = {\n",
    "            'total': n_total,\n",
    "            'train': len(train_files),\n",
    "            'val': len(val_files),\n",
    "            'test': len(test_files)\n",
    "        }\n",
    "\n",
    "    return split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DIR = '/content/drive/MyDrive/NHL_Project/data/lymphoma_split'\n",
    "\n",
    "print(\"Creating train/val/test split...\")\n",
    "print(\"This ensures NO data leakage between sets.\\n\")\n",
    "\n",
    "split_info = create_train_val_test_split(\n",
    "    source_dir=BASE_PATH,\n",
    "    dest_dir=SPLIT_DIR,\n",
    "    train_ratio=CONFIG['TRAIN_SPLIT'],\n",
    "    val_ratio=CONFIG['VAL_SPLIT'],\n",
    "    test_ratio=CONFIG['TEST_SPLIT'],\n",
    "    seed=CONFIG['RANDOM_SEED']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA SPLIT STATISTICS (NO LEAKAGE)\")\n",
    "print(\"=\"*70)\n",
    "for class_name, counts in split_info.items():\n",
    "    print(f\"\\n{CONFIG['CLASS_LABELS'].get(class_name, class_name)}:\")\n",
    "    print(f\"  Total: {counts['total']} images\")\n",
    "    print(f\"  Train: {counts['train']} ({counts['train']/counts['total']*100:.1f}%)\")\n",
    "    print(f\"  Val:   {counts['val']} ({counts['val']/counts['total']*100:.1f}%)\")\n",
    "    print(f\"  Test:  {counts['test']} ({counts['test']/counts['total']*100:.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✅ Split complete! Train/Val/Test are now completely independent.\")\n",
    "print(\"   Test set is held-out and will ONLY be used for final evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=list(split_info.keys()), y=[split_info[c]['total'] for c in split_info])\n",
    "plt.title(\"Class Distribution (Total Images per Class)\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xlabel(\"NHL Subtype\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Images Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "for i, class_dir in enumerate(split_info.keys()):\n",
    "    sample_img_path = os.path.join(BASE_PATH, class_dir, os.listdir(os.path.join(BASE_PATH, class_dir))[0])\n",
    "    img = cv2.imread(sample_img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(CONFIG['CLASS_LABELS'][class_dir])\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle(\"Sample Histopathology Images per NHL Subtype\", fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Split Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_folders = ['lymph_cll', 'lymph_fl', 'lymph_mcl']\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "split_counts = {}\n",
    "for split in splits:\n",
    "    split_path = os.path.join(SPLIT_DIR, split)\n",
    "    counts = {folder: len(os.listdir(os.path.join(split_path, folder))) for folder in class_folders}\n",
    "    split_counts[split] = counts\n",
    "\n",
    "df_counts = pd.DataFrame(split_counts).T\n",
    "df_counts.index.name = 'Split'\n",
    "df_counts.columns = ['CLL', 'FL', 'MCL']\n",
    "\n",
    "print(\"Image counts per split:\")\n",
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts.plot(kind='bar', figsize=(8,6))\n",
    "plt.title('Number of Images per Class in Each Split')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xlabel('Data Split')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = df_counts.loc['train']\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(train_counts, labels=train_counts.index, autopct='%1.1f%%', startangle=140, colors=['#ff9999','#66b3ff','#99ff99'])\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = {cls: [] for cls in class_folders}\n",
    "\n",
    "for cls in class_folders:\n",
    "    cls_path = os.path.join(SPLIT_DIR, 'train', cls)\n",
    "    for img_name in os.listdir(cls_path):\n",
    "        img_path = os.path.join(cls_path, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        image_sizes[cls].append(img.size)\n",
    "\n",
    "sizes_df = pd.DataFrame({cls: [h for w,h in image_sizes[cls]] for cls in class_folders})\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(data=sizes_df)\n",
    "plt.title('Image Height Distribution per Class (Train Set)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution per Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in splits:\n",
    "    counts = [len(os.listdir(os.path.join(SPLIT_DIR, split, cls))) for cls in class_folders]\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.pie(counts, labels=[cls.upper() for cls in class_folders], autopct='%1.1f%%', startangle=140)\n",
    "    plt.title(f'Class Distribution in {split.capitalize()} Set')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"✓ Data augmentation configured\")\n",
    "print(\"  Training: rotation, shift, zoom, horizontal flip\")\n",
    "print(\"  Val/Test: rescaling only (no augmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.join(SPLIT_DIR, 'train'),\n",
    "    target_size=CONFIG['IMG_SIZE'],\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=CONFIG['RANDOM_SEED']\n",
    ")\n",
    "\n",
    "validation_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(SPLIT_DIR, 'val'),\n",
    "    target_size=CONFIG['IMG_SIZE'],\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=CONFIG['RANDOM_SEED']\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    os.path.join(SPLIT_DIR, 'test'),\n",
    "    target_size=CONFIG['IMG_SIZE'],\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=CONFIG['RANDOM_SEED']\n",
    ")\n",
    "\n",
    "class_indices = train_generator.class_indices\n",
    "index_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA GENERATORS (FROM INDEPENDENT DIRECTORIES)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training samples:   {train_generator.samples}\")\n",
    "print(f\"Validation samples: {validation_generator.samples}\")\n",
    "print(f\"Test samples:       {test_generator.samples}\")\n",
    "print(f\"\\nClass mapping: {class_indices}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✅ All generators use SEPARATE directories - NO DATA LEAKAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "print(\"Class Weights (to handle imbalance):\")\n",
    "for idx, weight in class_weights.items():\n",
    "    print(f\"  {index_to_class[idx]}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable(package=\"Custom\")\n",
    "class TransformerBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation='gelu'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Hybrid Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hvit_nhl_model(input_shape=(128, 128, 3), num_classes=3,\n",
    "                          transformer_blocks=1, embed_dim=128, num_heads=2):\n",
    "    inputs = layers.Input(shape=input_shape, name='input_image')\n",
    "\n",
    "    base_model = MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "\n",
    "    H, W, C = x.shape[1], x.shape[2], x.shape[3]\n",
    "    x = layers.Reshape((H * W, C))(x)\n",
    "    x = layers.Dense(embed_dim)(x)\n",
    "\n",
    "    positions = tf.range(H * W)\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=H * W,\n",
    "        output_dim=embed_dim\n",
    "    )(positions)\n",
    "    x = x + position_embedding\n",
    "\n",
    "    for i in range(transformer_blocks):\n",
    "        x = TransformerBlock(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=embed_dim * 2\n",
    "        )(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='H-ViT-NHL')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Display Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = create_hvit_nhl_model(\n",
    "    input_shape=(*CONFIG['IMG_SIZE'], 3),\n",
    "    num_classes=CONFIG['NUM_CLASSES'],\n",
    "    transformer_blocks=1,\n",
    "    embed_dim=128,\n",
    "    num_heads=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL: Hybrid CNN + Vision Transformer (H-ViT-NHL)\")\n",
    "print(\"=\"*70)\n",
    "baseline_model.summary()\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainable_params = np.sum([tf.keras.backend.count_params(w)\n",
    "                          for w in baseline_model.trainable_weights])\n",
    "non_trainable_params = np.sum([tf.keras.backend.count_params(w)\n",
    "                               for w in baseline_model.non_trainable_weights])\n",
    "total_params = trainable_params + non_trainable_params\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"  Total:        {total_params:,}\")\n",
    "print(f\"  Trainable:    {trainable_params:,}\")\n",
    "print(f\"  Non-trainable: {non_trainable_params:,}\")\n",
    "print(f\"  Trainable ratio: {trainable_params/total_params*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=CONFIG['LEARNING_RATE']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.AUC(name='auc', multi_label=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline model compiled\")\n",
    "print(f\"  Optimizer: Adam (lr={CONFIG['LEARNING_RATE']})\")\n",
    "print(f\"  Loss: Categorical Cross-Entropy\")\n",
    "print(f\"  Metrics: Accuracy, AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f\"{MODEL_DIR}/best_hvit_baseline.keras\",\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✓ Training callbacks configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = baseline_model.fit(\n",
    "    train_generator,\n",
    "    epochs=CONFIG['EPOCHS'],\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"/content/drive/MyDrive/NHL_Project/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    axes[0].plot(history.history['loss'], label='Training', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1].plot(history.history['accuracy'], label='Training', linewidth=2)\n",
    "    axes[1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    axes[2].plot(history.history['auc'], label='Training', linewidth=2)\n",
    "    axes[2].plot(history.history['val_auc'], label='Validation', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('AUC', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_title('AUC', fontsize=14, fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history, \"Baseline H-ViT-NHL Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model and Create Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = keras.models.load_model(\n",
    "    f\"{MODEL_DIR}/best_hvit_baseline.keras\",\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "gap_layer = [layer for layer in baseline_model.layers if isinstance(layer, GlobalAveragePooling1D)][0]\n",
    "\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=baseline_model.input,\n",
    "    outputs=gap_layer.output\n",
    ")\n",
    "\n",
    "dummy_input = np.random.rand(1, 128, 128, 3).astype(np.float32)\n",
    "features = feature_extractor({'input_image': dummy_input})\n",
    "print(\"Feature shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating baseline on TRULY HELD-OUT test set...\\n\")\n",
    "\n",
    "test_steps = test_generator.samples // CONFIG['BATCH_SIZE']\n",
    "y_pred_probs_baseline = baseline_model.predict(test_generator, steps=test_steps, verbose=1)\n",
    "y_pred_classes_baseline = np.argmax(y_pred_probs_baseline, axis=1)\n",
    "y_true_classes = test_generator.classes[:len(y_pred_classes_baseline)]\n",
    "\n",
    "baseline_accuracy = np.mean(y_pred_classes_baseline == y_true_classes)\n",
    "\n",
    "print(f\"\\n✅ Baseline Test Accuracy: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
    "print(f\"   Evaluated on {len(y_pred_classes_baseline)} truly held-out test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_baseline = confusion_matrix(y_true_classes, y_pred_classes_baseline)\n",
    "cm_baseline_norm = confusion_matrix(y_true_classes, y_pred_classes_baseline, normalize='true')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CONFIG['CLASS_NAMES'],\n",
    "            yticklabels=CONFIG['CLASS_NAMES'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(cm_baseline_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=CONFIG['CLASS_NAMES'],\n",
    "            yticklabels=CONFIG['CLASS_NAMES'],\n",
    "            ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Baseline H-ViT-NHL Performance (Held-Out Test Set)',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm_baseline_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=CONFIG['CLASS_NAMES'], yticklabels=CONFIG['CLASS_NAMES'])\n",
    "plt.title(\"Baseline H-ViT Confusion Matrix (Normalized)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE CLASSIFICATION REPORT (HELD-OUT TEST SET)\")\n",
    "print(\"=\"*70)\n",
    "report_baseline = classification_report(\n",
    "    y_true_classes,\n",
    "    y_pred_classes_baseline,\n",
    "    target_names=CONFIG['CLASS_NAMES'],\n",
    "    digits=4\n",
    ")\n",
    "print(report_baseline)\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_precision, baseline_recall, baseline_f1, _ = precision_recall_fscore_support(\n",
    "    y_true_classes, y_pred_classes_baseline, average='macro'\n",
    ")\n",
    "\n",
    "y_true_bin = label_binarize(y_true_classes, classes=[0, 1, 2])\n",
    "\n",
    "baseline_roc_auc = {}\n",
    "for i in range(CONFIG['NUM_CLASSES']):\n",
    "    baseline_roc_auc[i] = roc_auc_score(y_true_bin[:, i], y_pred_probs_baseline[:, i])\n",
    "\n",
    "baseline_macro_auc = roc_auc_score(y_true_bin, y_pred_probs_baseline, average='macro')\n",
    "\n",
    "print(\"\\nBaseline ROC-AUC (Held-Out Test):\")\n",
    "for i, class_name in enumerate(CONFIG['CLASS_NAMES']):\n",
    "    print(f\"  {class_name}: {baseline_roc_auc[i]:.4f}\")\n",
    "print(f\"  Macro-Average: {baseline_macro_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "for i, class_name in enumerate(CONFIG['CLASS_NAMES']):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_probs_baseline[:, i])\n",
    "    plt.plot(fpr, tpr, label=f\"{class_name} (AUC = {baseline_roc_auc[i]:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves - Baseline H-ViT\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = feature_extractor.predict(test_generator, steps=test_steps, verbose=1)\n",
    "\n",
    "y_true = test_generator.classes[:X_features.shape[0]]\n",
    "\n",
    "n_samples = X_features.shape[0]\n",
    "perplexity = min(30, max(5, n_samples // 3))\n",
    "\n",
    "print(f\"t-SNE: n_samples={n_samples}, using perplexity={perplexity}\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "X_2d = tsne.fit_transform(X_features)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for idx, class_name in enumerate(CONFIG['CLASS_NAMES']):\n",
    "    mask = (y_true == idx)\n",
    "    plt.scatter(X_2d[mask, 0], X_2d[mask, 1], label=class_name, alpha=0.7, s=50)\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Test Set Features\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
